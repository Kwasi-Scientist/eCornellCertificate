{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>About this Project</h2>\n",
       "<p>In this Project, you will find the gradient of the logistic loss function and implement gradient descent in order to create a linear classifier that can \"filter\" spam email messages.</p>\n",
       "\n",
       "<h3>Evaluation</h3>\n",
       "\n",
       "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
       "    \n",
       "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
       "    \n",
       "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board (found in the Live Labs section of this course) to engage with your peers or seek assistance from the instructor.<p>\n",
       "\n",
       "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
       "\n",
       "<h3>Submit Code for Autograder Feedback</h3>\n",
       "\n",
       "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
       "\n",
       "<ol>\n",
       "    <li><strong>Save your notebook —</strong> Click <strong>Save and Checkpoint</strong> in the \"File\" menu.</li>\n",
       "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
       "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
       "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
       "</ol>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>About this Project</h2>\n",
    "<p>In this Project, you will find the gradient of the logistic loss function and implement gradient descent in order to create a linear classifier that can \"filter\" spam email messages.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board (found in the Live Labs section of this course) to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "    <li><strong>Save your notebook —</strong> Click <strong>Save and Checkpoint</strong> in the \"File\" menu.</li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Getting Started</h2>\n",
       "<h3>Python Initialization</h3> \n",
       "\n",
       "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pylab import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from helper import *\n",
    "\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next, let's simulate some 2D data with binary (0/1) labels.  You'll be generating this data from non-overlapping multivariate normal distributions that should be very easily separable for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n_samples = 500\n",
    "\n",
    "class_one = np.random.multivariate_normal([5, 10], [[1, .25],[.25, 1]], n_samples)\n",
    "class_one_labels = -np.ones(n_samples)\n",
    "\n",
    "class_two = np.random.multivariate_normal([0, 5], [[1, .25],[.25, 1]], n_samples)\n",
    "class_two_labels = np.ones(n_samples)\n",
    "\n",
    "features = np.vstack((class_one, class_two))\n",
    "labels = np.hstack((class_one_labels, class_two_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what what our feature arrays look like. \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize these data distributions\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = labels, alpha = .6);\n",
    "\n",
    "plt.title(\"Binary labeled data in 2D\", size=15);\n",
    "plt.xlabel(\"Feature 1\", size=13);\n",
    "plt.ylabel(\"Feature 2\", size=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>In logistic regression, we use gradient ascent to solve for the weight vector that maximizes the (log) likelihood of observing the data.  (Equivalently, we can use gradient _descent_ to solve for the weight vector that _minimizes_ the _negative_ log likelihood - refer to Module 3 if you need a review of this derivation!)</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<p>In logistic regression, we use gradient ascent to solve for the weight vector that maximizes the (log) likelihood of observing the data.  (Equivalently, we can use gradient _descent_ to solve for the weight vector that _minimizes_ the _negative_ log likelihood - refer to Module 3 if you need a review of this derivation!)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Gradient Descent and Email Spam Classification</h2>\n",
       "\n",
       "<h3>Part One: Sigmoid [Graded]</h3>\n",
       "\n",
       "<p>To begin, you must first implement the sigmoid function:    $\\sigma(z)=\\frac{1}{1+e^{-z}}$ </p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>Gradient Descent and Email Spam Classification</h2>\n",
    "\n",
    "<h3>Part One: Sigmoid [Graded]</h3>\n",
    "\n",
    "<p>To begin, you must first implement the sigmoid function:    $\\sigma(z)=\\frac{1}{1+e^{-z}}$ </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Input: \n",
    "    # z : scalar or array of dimension n \n",
    "    # Output:\n",
    "    # sgmd: scalar or array of dimension n\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    sgmd = 1/(1+np.exp(-z))\n",
    "    sgmd\n",
    "    return sgmd\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# h = np.random.rand(10) # input is an 10-dimensional array\n",
    "# sgmd1 = sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid1():\n",
    "    h = np.random.rand(10) # input is an 10-dimensional array\n",
    "    sgmd1 = sigmoid(h)\n",
    "    return sgmd1.shape == h.shape # output should be a 10-dimensional array\n",
    "\n",
    "def test_sigmoid2():\n",
    "    h = np.random.rand(10) # input is an 10-dimensional array\n",
    "    sgmd1 = sigmoid(h) # compute the sigmoids with your function\n",
    "    sgmd2 = sigmoid_grader(h) # compute the sigmoids with ground truth funcdtion\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5) # check if they agree\n",
    "\n",
    "def test_sigmoid3():\n",
    "    x = np.random.rand(1) # input is a scalar\n",
    "    sgmd1 = sigmoid(x) # compute the sigmoids with your function\n",
    "    sgmd2 = sigmoid_grader(x) # compute the sigmoids with ground truth function\n",
    "    return (np.linalg.norm(sgmd1 - sgmd2) < 1e-5) # check if they agree\n",
    "\n",
    "def test_sigmoid4():\n",
    "    x = np.array([-1e10,1e10,0]) # three input points: very negative, very positive, and 0\n",
    "    sgmds = sigmoid(x) # compute the sigmoids with your function\n",
    "    truth = np.array([0,1,0.5]) # the truth should be 0, 1, 0.5 exactly\n",
    "    return (np.linalg.norm(sgmds - truth) < 1e-8) # test if this is true\n",
    "\n",
    "\n",
    "runtest(test_sigmoid1, 'test_sigmoid1')\n",
    "runtest(test_sigmoid2, 'test_sigmoid2')\n",
    "runtest(test_sigmoid3, 'test_sigmoid3')\n",
    "runtest(test_sigmoid4, 'test_sigmoid4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Part Two: Implement <code>y_pred</code> [Graded]</h3>\n",
       "\n",
       "<p>We consider binary logistic regression with class labels $y\\in\\{+1,-1\\}$. Implement a function <code>y_pred(X,w)</code> that computes $P(y=1\\;|\\;\\mathbf{x};\\mathbf{w}, b)$ for each row-vector $\\mathbf{x}$ in the matrix <code>X</code>.</p>\n",
       "<p>Recall that:\n",
       "$$P(y\\;|\\;\\mathbf{x};\\mathbf{w})=\\sigma(y (\\mathbf{w}^\\top \\mathbf{x} + b))$$\n",
       "</p>\n",
       "<br />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Part Two: Implement <code>y_pred</code> [Graded]</h3>\n",
    "\n",
    "<p>We consider binary logistic regression with class labels $y\\in\\{+1,-1\\}$. Implement a function <code>y_pred(X,w)</code> that computes $P(y=1\\;|\\;\\mathbf{x};\\mathbf{w}, b)$ for each row-vector $\\mathbf{x}$ in the matrix <code>X</code>.</p>\n",
    "<p>Recall that:\n",
    "$$P(y\\;|\\;\\mathbf{x};\\mathbf{w})=\\sigma(y (\\mathbf{w}^\\top \\mathbf{x} + b))$$\n",
    "</p>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred(X, w, b=0):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # w: d-dimensional vector\n",
    "    # b: scalar (optional, if not passed on is treated as 0)\n",
    "    # Output:\n",
    "    # prob: n-dimensional vector\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    prob = sigmoid(np.inner(X,w.T) + b)\n",
    "    \n",
    "    \n",
    "    #print('X: ', X.shape)\n",
    "    #print('W: ', w.shape)\n",
    "    #print('prob: ', prob.shape)\n",
    "    return prob\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "n = 20\n",
    "d = 5\n",
    "X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(5) # define a random weight vector\n",
    "probs=y_pred(X,w,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ypred1():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs=y_pred(X,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return probs.shape == (n, ) # check if all outputs are >=0 and <=1\n",
    "\n",
    "\n",
    "def test_ypred2():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return all(probs>=0) and all(probs<=1) # check if all outputs are >=0 and <=1\n",
    "\n",
    "def test_ypred3():\n",
    "    n = 20\n",
    "    d = 5\n",
    "    X = np.random.rand(n,d) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    probs1=y_pred(X, w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    probs2=y_pred(X,-w, 0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return np.linalg.norm(probs1+probs2-1)<1e-08 # check if P(y|x;w)+P(y|x;-w)=1\n",
    "\n",
    "\n",
    "\n",
    "def test_ypred4():\n",
    "    X=np.random.rand(25,4) # define random input\n",
    "    w=np.array([1,0,0,0]) # all-zeros weight vector\n",
    "    prob=y_pred(X, w, 0) # compute P(y|X;w)\n",
    "    truth=sigmoid(X[:,0]) # should simply be the sigmoid of the first feature\n",
    "    return np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "\n",
    "def test_ypred5(): \n",
    "    X=np.array([[0.61793598, 0.09367891], # define 3 inputs (2D)\n",
    "               [0.79447745, 0.98605996],\n",
    "               [0.53679997, 0.4253885 ]])\n",
    "    w=np.array([0.9822789 , 0.16017851]); # define weight vector\n",
    "    prob=y_pred(X, w, 3) # compute P(y|X;w)\n",
    "    truth=np.array([0.97396645,0.98089179,0.97328431]) # this is the grount truth\n",
    "    return np.linalg.norm(prob-truth)<1e-08 # see if they match\n",
    "\n",
    "runtest(test_ypred1, 'test_ypred1')\n",
    "runtest(test_ypred2, 'test_ypred2')\n",
    "runtest(test_ypred3, 'test_ypred3')\n",
    "runtest(test_ypred4, 'test_ypred4')\n",
    "runtest(test_ypred5, 'test_ypred5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Part Three: Implement <code>log_loss</code> [Graded]</h3>\n",
       "\n",
       "<p>Assume you are given a data set $\\mathbf{x}_1,\\dots,\\mathbf{x}_n$ stored as row-vectors in a matrix <code>X</code> with labels $y_1,\\dots,y_n$ stored in vector <code>y</code>. Compute the <b>negative</b> log-likelihood (<code>log_loss</code>) of all inputs\n",
       "    $$NLL=-\\log P(\\mathbf{y}\\;|\\;\\mathbf{X};\\mathbf{w}, b)=-\\sum_{i=1}^n \\log\\left((P(y_i\\;|\\; \\mathbf{x}_i;\\mathbf{w}, b))\\right)=-\\sum_{i=1}^n \\log\\left(\\sigma(y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b))\\right).$$\n",
       "    </p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Part Three: Implement <code>log_loss</code> [Graded]</h3>\n",
    "\n",
    "<p>Assume you are given a data set $\\mathbf{x}_1,\\dots,\\mathbf{x}_n$ stored as row-vectors in a matrix <code>X</code> with labels $y_1,\\dots,y_n$ stored in vector <code>y</code>. Compute the <b>negative</b> log-likelihood (<code>log_loss</code>) of all inputs\n",
    "    $$NLL=-\\log P(\\mathbf{y}\\;|\\;\\mathbf{X};\\mathbf{w}, b)=-\\sum_{i=1}^n \\log\\left((P(y_i\\;|\\; \\mathbf{x}_i;\\mathbf{w}, b))\\right)=-\\sum_{i=1}^n \\log\\left(\\sigma(y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b))\\right).$$\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(X, y, w, b=0):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # w: d-dimensional vector\n",
    "    # Output:\n",
    "    # a scalar\n",
    "    assert np.sum(np.abs(y))==len(y) # check if all labels in y are either +1 or -1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #W = (X.T.dot(y))/(X.T.dot(X))\n",
    "    #print('W: ', W.shape)\n",
    "    #print('y', y.shape)\n",
    "    \n",
    "   # nll = -np.sum(np.log(1 + np.exp(-y*np.inner(w.T,X)))\n",
    "   # nll = -np.sum(np.log(1-y_pred(X,w,b)))\n",
    "    #score = np.dot(X,w)\n",
    "    #nll = -np.sum(y*scores - np.log(1+ np.exp(scores)))\n",
    "    #nll = -np.log(y_pred(X,w,b))\n",
    "    #nll = (-y * np.log(y_pred(X,w,b)) - (1 - y) * np.log(1 - y_pred(X,w,b))).mean() \n",
    "    #z = np.dot(X, w)\n",
    "    #nll = -np.sum( y*z - np.log(1 + np.exp(z)) )\n",
    "    \n",
    "    #nll = -np.sum(np.log(y_pred(X,w,b)))\n",
    "    #nll = np.sum(-y*np.log(y_pred(X,w,b)) - (1 - y)*np.log(1 - y_pred(X,w,b)))\n",
    "    #nll = -np.sum(y@np.log(y_pred(X,w,b)))\n",
    "\n",
    "    #nll = -np.sum(np.log(y_pred(X,w,b)))\n",
    "\n",
    "    nll3 = -np.sum(np.log(sigmoid(y*(np.inner(w.T,X) +b))))\n",
    "    #nll2 = -np.sum(np.log(sigmoid(y@np.inner(w.T,X)+b)))\n",
    "    \n",
    "    #print('y: ', y.shape)\n",
    "    #print('Partial: ', nll3)     \n",
    "    #print('nll2: ', nll2)              \n",
    "    #print('nll: ', nll)\n",
    "    return nll3\n",
    "    raise NotImplementedError()\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(500,15) # generate n random vectors with d dimensions\n",
    "w = np.random.rand(15) # define a random weight vector\n",
    "b = np.random.rand(1) # define a bias\n",
    "y = (np.random.rand(500)>0.5)*2-1;\n",
    "ll=log_loss(X,y,w, b) #\n",
    "ll2 = log_loss_grader(X,y,w,b)\n",
    "print('ll2: ', ll2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logloss1():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = np.ones(25) # set labels all-(+1)\n",
    "    ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    # if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "    return np.isscalar(ll) # check whether the output is a scalar\n",
    "\n",
    "def test_logloss2():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = np.ones(25) # set labels all-(+1)\n",
    "    ll=log_loss(X,y,w, b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=-np.sum(np.log(y_pred(X, w, b))) # if labels are all-ones function becomes simply the sum of log of y_pred\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "def test_logloss3():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = -np.ones(25) # set labels all-(-1)\n",
    "    ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=-np.sum(np.log(1-y_pred(X,w, b))) # if labels are all-ones function becomes simply the sum of log of 1-y_pred\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "def test_logloss4():\n",
    "    X = np.random.rand(20,5) # generate n random vectors with d dimensions\n",
    "    w = np.array([0,0,0,0,0]) # define an all-zeros weight vector\n",
    "    y = (np.random.rand(20)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "    ll=log_loss(X,y,w,0) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    # the log-likelihood for each of the 20 examples should be exactly 0.5:\n",
    "    return np.linalg.norm(ll+20*np.log(0.5))<1e-08 \n",
    "\n",
    "def test_logloss5():\n",
    "    X = np.random.rand(500,15) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(15) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(500)>0.5)*2-1; # define n random labels (+1 or -1)\n",
    "    ll=log_loss(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    ll2=log_loss_grader(X,y,w,b) # compute the probabilities of P(y=1|x;w) using your y_pred function\n",
    "    return np.linalg.norm(ll-ll2)<1e-05\n",
    "\n",
    "runtest(test_logloss1, 'test_logloss1')\n",
    "runtest(test_logloss2, 'test_logloss2')\n",
    "runtest(test_logloss3, 'test_logloss3')\n",
    "runtest(test_logloss4, 'test_logloss4')\n",
    "runtest(test_logloss5, 'test_logloss5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Part Four: Compute Gradient [Graded]</h3>\n",
       "\n",
       "Now, verify that the gradient of the log-loss with respect to the weight vector is:\n",
       "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial \\mathbf{w}}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b))\\mathbf{x}_i.$$ \n",
       "\n",
       "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial b}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b)).$$\n",
       "Implement the function <code>gradient</code> which returns the first derivative with respect to <code>w, b</code> for a given <code>X, y, w, b</code>.\n",
       "\n",
       "Hint: remember that you derived earlier that $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Part Four: Compute Gradient [Graded]</h3>\n",
    "\n",
    "Now, verify that the gradient of the log-loss with respect to the weight vector is:\n",
    "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial \\mathbf{w}}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b))\\mathbf{x}_i.$$ \n",
    "\n",
    "$$\\frac{\\partial NLL(\\mathbf{X},\\mathbf{y},\\mathbf{w}, b)}{\\partial b}=\\sum_{i=1}^n -y_i\\sigma(-y_i(\\mathbf{w}^\\top\\mathbf{x}_i + b)).$$\n",
    "Implement the function <code>gradient</code> which returns the first derivative with respect to <code>w, b</code> for a given <code>X, y, w, b</code>.\n",
    "\n",
    "Hint: remember that you derived earlier that $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, w, b):\n",
    "    # Input:\n",
    "    # X: nxd matrix\n",
    "    # y: n-dimensional vector with labels (+1 or -1)\n",
    "    # w: d-dimensional vector\n",
    "    # b: a scalar bias term\n",
    "    # Output:\n",
    "    # wgrad: d-dimensional vector with gradient\n",
    "    # bgrad: a scalar with gradient\n",
    "    \n",
    "    n, d = X.shape\n",
    "    wgrad = np.zeros(d)\n",
    "    bgrad = 0.0\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    h = y - y_pred(X,w,b)\n",
    "    #wgrad = np.sum*y-y_pred(X,w,b)*X\n",
    "    \n",
    "    #wgrad = np.dot(X.T,h)\n",
    "    wgrad = -y*(sigmoid(-y*(np.inner(w.T,X) +b)))@X\n",
    "\n",
    "    partialx = -y*(sigmoid(-y*(np.inner(w.T,X) +b)))@X\n",
    "    bgrad = np.sum(-y*(sigmoid(-y*(np.inner(w.T,X) +b))))\n",
    "\n",
    "\n",
    "    \n",
    "    return wgrad, bgrad\n",
    " \n",
    "# X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "# w = np.random.rand(5) # define a random weight vector\n",
    "# b = np.random.rand(1) # define a bias\n",
    "# y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "# wgrad, bgrad = gradient(X, y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_grad1():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "    \n",
    "    return wgrad.shape == w.shape and np.isscalar(bgrad)\n",
    "\n",
    "\n",
    "def test_grad2():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) # define a bias\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    wgrad, bgrad = gradient(X, y, w, b) # compute the gradient using your function\n",
    "    wgrad2, bgrad2 = gradient_grader(X, y, w, b) # compute the gradient using ground truth\n",
    "    return np.linalg.norm(wgrad - wgrad2)<1e-06 and np.linalg.norm(bgrad - bgrad2) < 1e-06 # test if they match\n",
    "\n",
    "def test_grad3():\n",
    "    X = np.random.rand(25,5) # generate n random vectors with d dimensions\n",
    "    y = (np.random.rand(25)>0.5)*2-1 # set labels all-(+1)\n",
    "    w = np.random.rand(5) # define a random weight vector\n",
    "    b = np.random.rand(1) \n",
    "\n",
    "    w_s = np.random.rand(5)*1e-05 # define tiny random step \n",
    "    b_s = np.random.rand(1)*1e-05 # define tiny random step \n",
    "    ll1 = log_loss(X,y,w+w_s, b+b_s)  # compute log-likelihood after taking a step\n",
    "    \n",
    "    ll = log_loss(X,y,w,b) # use Taylor's expansion to approximate new loss with gradient\n",
    "    wgrad, bgrad =gradient(X,y,w,b) # compute gradient\n",
    "    ll2=ll+ wgrad @ w_s + bgrad * b_s # take linear step with Taylor's approximation\n",
    "    return np.linalg.norm(ll1-ll2)<1e-05 # test if they match\n",
    "\n",
    "def test_grad4():\n",
    "    w1, b1, losses1 = logistic_regression_grader(features, labels, 1000, 1e-03, gradient)\n",
    "    w2, b2, losses2 = logistic_regression_grader(features, labels, 1000, 1e-03)\n",
    "    return(np.abs(losses1[-1]-losses2[-1])<0.1)\n",
    "\n",
    "runtest(test_grad1, 'test_grad1')\n",
    "runtest(test_grad2, 'test_grad2')\n",
    "runtest(test_grad3, 'test_grad3')\n",
    "runtest(test_grad4, 'test_grad4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Part Five: Weight Update of Gradient Ascent [Graded]</h3>\n",
       "    \n",
       "Write code below to implement the weight update of gradient descent on the log_loss function.  Hint: use the `gradient` and `log_loss` functions from above. Please use a <b>constant</b> learning rate throughout (i.e. do not decrease the learning rate).\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Part Five: Weight Update of Gradient Ascent [Graded]</h3>\n",
    "    \n",
    "Write code below to implement the weight update of gradient descent on the log_loss function.  Hint: use the `gradient` and `log_loss` functions from above. Please use a <b>constant</b> learning rate throughout (i.e. do not decrease the learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, max_iter, alpha):\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "    #losses = np.zeros(max_iter)    \n",
    "    losses = []\n",
    "    for step in range(max_iter):\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        #Get wgradient and b gradient\n",
    "        \n",
    "        wgrad, bgrad = gradient(X,y, w,b)\n",
    "        \n",
    "        #update w\n",
    "        w = w - alpha*wgrad\n",
    "        \n",
    "        #update b\n",
    "        b = b -alpha*bgrad\n",
    "        \n",
    "        #define losses\n",
    "        losses.append(log_loss(X,y,w,b))\n",
    "        \n",
    "    \n",
    "    return w, b, losses\n",
    "\n",
    "weight, b, losses = logistic_regression(features, labels, 1000, 1e-04)\n",
    "plot(losses)\n",
    "xlabel('iterations')\n",
    "ylabel('log_loss')\n",
    "# your loss should go down :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression1():\n",
    "\n",
    "    XUnit = np.array([[-1,1],[-1,0],[0,-1],[-1,2],[1,-2],[1,-1],[1,0],[0,1],[1,-2],[-1,2]])\n",
    "    YUnit = np.hstack((np.ones(5), -np.ones(5)))\n",
    "\n",
    "    w1, b1, _ = logistic_regression(XUnit, YUnit, 30000, 5e-5)\n",
    "    w2, b2, _ = logistic_regression_grader(XUnit, YUnit, 30000, 5e-5)\n",
    "    return (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "def test_logistic_regression2():\n",
    "    X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "    Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "    max_iter = 300\n",
    "    alpha = 1e-5\n",
    "    w1, b1, _ = logistic_regression(X, Y, max_iter, alpha)\n",
    "    w2, b2, _ = logistic_regression_grader(X, Y, max_iter, alpha)\n",
    "    return (np.linalg.norm(w1 - w2) < 1e-5) and (np.linalg.norm(b1 - b2) < 1e-5)\n",
    "\n",
    "def test_logistic_regression3(): # check if losses match predictions\n",
    "    X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "    Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "    max_iter = 30\n",
    "    alpha = 1e-5\n",
    "    w1, b1, losses1 = logistic_regression(X, Y, max_iter, alpha)\n",
    "    return np.abs(log_loss(X,Y,w1,b1)-losses1[-1])<1e-09\n",
    "\n",
    "def test_logistic_regression4(): # check if loss decreases\n",
    "    X = np.vstack((np.random.randn(50, 5), np.random.randn(50, 5) + 2))\n",
    "    Y = np.hstack((np.ones(50), -np.ones(50)))\n",
    "    max_iter = 30\n",
    "    alpha = 1e-5\n",
    "    w1, b1, losses1 = logistic_regression(X, Y, max_iter, alpha)\n",
    "    return losses[-1]<losses[0]\n",
    "\n",
    "runtest(test_logistic_regression1, 'test_logistic_regression1')\n",
    "runtest(test_logistic_regression2, 'test_logistic_regression2')\n",
    "runtest(test_logistic_regression3, 'test_logistic_regression3')\n",
    "runtest(test_logistic_regression4, 'test_logistic_regression4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Original Data</h2>\n",
       "<p>Now run your implementation on the binary classification data from the top of the notebook.  Check your code by plotting the values of the negative log likelihood - should these values increase or decrease as the number of iterations grows?  Do your values move in the right direction?</p>\n",
       "\n",
       "You can tune `max_iter` and `alpha` to see how they affect convergence!  \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>Original Data</h2>\n",
    "<p>Now run your implementation on the binary classification data from the top of the notebook.  Check your code by plotting the values of the negative log likelihood - should these values increase or decrease as the number of iterations grows?  Do your values move in the right direction?</p>\n",
    "\n",
    "You can tune `max_iter` and `alpha` to see how they affect convergence!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "alpha = 1e-4\n",
    "final_w, final_b, losses = logistic_regression(features, labels, max_iter, alpha)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. iteration\", size=15)\n",
    "plt.xlabel(\"Num iteration\", size=13)\n",
    "plt.ylabel(\"Loss value\", size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Below, we will take the final weights from the logistic solver and predict labels for the entire dataset.  By plotting the results, we can get a sense of where the linear decision boundary lies.  What do you notice?  What could be changed to further improve the accuracy of the classifier? (_Hint: take a look at the second video in Module 1._)</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<p>Below, we will take the final weights from the logistic solver and predict labels for the entire dataset.  By plotting the results, we can get a sense of where the linear decision boundary lies.  What do you notice?  What could be changed to further improve the accuracy of the classifier? (_Hint: take a look at the second video in Module 1._)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = y_pred(features, final_w, final_b)\n",
    "\n",
    "pred_labels = (scores > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# plot the decision boundary \n",
    "x = np.linspace(np.amin(features[:, 0]), np.amax(features[:, 0]), 10)\n",
    "y = -(final_w[0] * x + final_b)/ final_w[1] \n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.scatter(features[:, 0], features[:, 1],\n",
    "            c = pred_labels, alpha = .6)\n",
    "plt.title(\"Predicted labels\", size=15)\n",
    "plt.xlabel(\"Feature 1\", size=13)\n",
    "plt.ylabel(\"Feature 2\", size=13)\n",
    "plt.axis([-3,10,0,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Build an Email Spam Filter</h1>\n",
       "<p> With logistic regression implemented, you can now build an email spam filter using logistic regression. The functions below load in pre-processed email data from cloud storage, where emails are represented as one-hot vectors.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h1>Build an Email Spam Filter</h1>\n",
    "<p> With logistic regression implemented, you can now build an email spam filter using logistic regression. The functions below load in pre-processed email data from cloud storage, where emails are represented as one-hot vectors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import dask.bag\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "\n",
    "train_url = 's3://codio/CIS530/CIS533/data_train'\n",
    "test_url = 's3://codio/CIS530/CIS533/data_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extract_features_naive(email, B):\n",
    "    # initialize all-zeros feature vector\n",
    "    v = np.zeros(B)\n",
    "    email = ' '.join(email)\n",
    "    # breaks for non-ascii characters\n",
    "    tokens = email.split()\n",
    "    for token in tokens:\n",
    "        v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def load_spam_data(extract_features, B=512, url=train_url):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    \n",
    "    all_emails = pd.read_csv(url+'/index', header=None).values.flatten()\n",
    "    \n",
    "    xs = np.zeros((len(all_emails), B))\n",
    "    ys = np.zeros(len(all_emails))\n",
    "    \n",
    "    labels = [k.split()[0] for k in all_emails]\n",
    "    paths = [url+'/'+k.split()[1] for k in all_emails]\n",
    "\n",
    "    ProgressBar().register()\n",
    "    dask.config.set(scheduler='threads', num_workers=50)\n",
    "    bag = dask.bag.read_text(paths, storage_options={'anon':True})\n",
    "    contents = dask.bag.compute(*bag.to_delayed())\n",
    "    for i, email in enumerate(contents):\n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (labels[i] == 'spam') * 2 - 1\n",
    "        xs[i, :] = extract_features(email, B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "Xspam, Yspam = load_spam_data(extract_features_naive)\n",
    "Xspam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Split The Dataset</h3>\n",
       "\n",
       "<p>Now that you have loaded the dataset, it's time to split it into training and testing. To evaluate your algorithm, run the code below to split off 20% of this data into a testing set, leaving 80% as your training set.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Split The Dataset</h3>\n",
    "\n",
    "<p>Now that you have loaded the dataset, it's time to split it into training and testing. To evaluate your algorithm, run the code below to split off 20% of this data into a testing set, leaving 80% as your training set.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (xTr and yTr) \n",
    "# and testing (xTv and yTv)\n",
    "n, d = Xspam.shape\n",
    "# Allocate 80% of the data for training and 20% for testing\n",
    "cutoff = int(np.ceil(0.8 * n))\n",
    "# indices of training samples\n",
    "xTr = Xspam[:cutoff,:]\n",
    "yTr = Yspam[:cutoff]\n",
    "# indices of testing samples\n",
    "xTv = Xspam[cutoff:]\n",
    "yTv = Yspam[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Training and Evaluating </h3>\n",
       "\n",
       "<p> Running the following cell will produce a logistic regression model that can classify unseen emails at roughly 88% accuracy</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Training and Evaluating </h3>\n",
    "\n",
    "<p> Running the following cell will produce a logistic regression model that can classify unseen emails at roughly 88% accuracy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5000\n",
    "alpha = 1e-5\n",
    "final_w_spam, final_b_spam, losses = logistic_regression(xTr, yTr, max_iter, alpha)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss vs. iteration\", size=15)\n",
    "plt.xlabel(\"Num iteration\", size=13)\n",
    "plt.ylabel(\"Loss value\", size=13)\n",
    "\n",
    "# evaluate training accuracy\n",
    "scoresTr = y_pred(xTr, final_w_spam, final_b_spam)\n",
    "pred_labels = (scoresTr > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "trainingacc = np.mean(pred_labels == yTr)\n",
    "\n",
    "# evaluate testing accuracy\n",
    "scoresTv = y_pred(xTv, final_w_spam, final_b_spam)\n",
    "pred_labels = (scoresTv > 0.5).astype(int)\n",
    "pred_labels[pred_labels != 1] = -1\n",
    "validationacc = np.mean(pred_labels==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Challenge: Improve Your Spam Classifier <b>[Ungraded]</b></h2>\n",
       "\n",
       "<p>You can improve your classifier in two ways:</p>\n",
       "\n",
       "<ol>\n",
       "<li><b>Feature Extraction</b>:\n",
       "Modify the function <code>extract_features_comp()</code>.\n",
       "This function takes in a file path <code>path</code> and\n",
       "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
       "The autograder will pass in a file path pointing to a file that contains an email,\n",
       "and set <code>B</code> = <code>feature_dimension</code>.\n",
       "We provide a naive feature extraction as an example.\n",
       "</li>\n",
       "<li><b>Model Training</b>:\n",
       "Modify the function <code>train_spam_filter_comp()</code>.\n",
       "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
       "should output a weight vector <code>w</code> and bias <code>b</code> for linear classification. The predictions will be calculated exactly the same way as we have demonstrated in the previous cell. \n",
       "We provide an initial implementation using gradient descent and logistic regression.\n",
       "</li>\n",
       "</ol>\n",
       "\n",
       "<p>Your model will be trained on the same training set above (loaded by <code>load_spam_data()</code>), but we will test its accuracy on a secret dataset of emails.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2>Challenge: Improve Your Spam Classifier <b>[Ungraded]</b></h2>\n",
    "\n",
    "<p>You can improve your classifier in two ways:</p>\n",
    "\n",
    "<ol>\n",
    "<li><b>Feature Extraction</b>:\n",
    "Modify the function <code>extract_features_comp()</code>.\n",
    "This function takes in a file path <code>path</code> and\n",
    "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
    "The autograder will pass in a file path pointing to a file that contains an email,\n",
    "and set <code>B</code> = <code>feature_dimension</code>.\n",
    "We provide a naive feature extraction as an example.\n",
    "</li>\n",
    "<li><b>Model Training</b>:\n",
    "Modify the function <code>train_spam_filter_comp()</code>.\n",
    "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
    "should output a weight vector <code>w</code> and bias <code>b</code> for linear classification. The predictions will be calculated exactly the same way as we have demonstrated in the previous cell. \n",
    "We provide an initial implementation using gradient descent and logistic regression.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Your model will be trained on the same training set above (loaded by <code>load_spam_data()</code>), but we will test its accuracy on a secret dataset of emails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dimension = 512\n",
    "def extract_features_challenge(email, B=feature_dimension):\n",
    "    '''\n",
    "    INPUT:\n",
    "    path : file path of email\n",
    "    B    : dimensionality of feature vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    x    : B dimensional vector\n",
    "    '''\n",
    "    # initialize all-zeros feature vector\n",
    "    v = np.zeros(B)\n",
    "    email = ' '.join(email)\n",
    "    # breaks for non-ascii characters\n",
    "    tokens = email.split()\n",
    "    for token in tokens:\n",
    "        v[hash(token) % B] = 1\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_spam_filter_challenge(xTr, yTr):\n",
    "    '''\n",
    "    INPUT:\n",
    "    xTr : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr : d   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w : d dimensional vector for linear classification\n",
    "    '''\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    max_iter = 100\n",
    "    alpha = 1e-5\n",
    "    w, b, losses = logistic_regression(xTr, yTr, max_iter, alpha)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenge_selftest():\n",
    "    xTr,yTr = load_spam_data(extract_features_challenge, feature_dimension, train_url)\n",
    "    w, b = train_spam_filter_challenge(xTr, yTr)\n",
    "    xTe,yTe = load_spam_data(extract_features_challenge, feature_dimension, test_url)\n",
    "    scoresTe = sigmoid_grader(xTe @ w + b)\n",
    "    \n",
    "    preds = (scoresTe > 0.5).astype(int)\n",
    "    preds[preds != 1] = -1\n",
    "    \n",
    "    pos_ind = (yTe == 1)\n",
    "    neg_ind = (yTe == -1)\n",
    "    \n",
    "    pos_acc = np.mean(yTe[pos_ind] == preds[pos_ind])\n",
    "    neg_acc = np.mean(yTe[neg_ind] == preds[neg_ind])\n",
    "    \n",
    "    test_accuracy = 0.5*pos_acc + 0.5*neg_acc\n",
    "    \n",
    "    scoresTr =  sigmoid_grader(xTr @ w + b)\n",
    "    preds_Tr = (scoresTr > 0.5).astype(int)\n",
    "    preds_Tr[preds_Tr != 1] = -1\n",
    "    \n",
    "    training_accuracy = np.mean(preds_Tr == yTr)\n",
    "    return training_accuracy, test_accuracy\n",
    "\n",
    "training_acc, test_acc = challenge_selftest()\n",
    "print(\"Your features and model achieved training accuracy: {:.2f}% and test accuracy: {:.2f}%\".format(training_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
